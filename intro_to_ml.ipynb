{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and AI Introduction for New Orleanians\n",
    "\n",
    "## Introduction to Artificial Intelligence and Machine Learning\n",
    "\n",
    "### What is Artificial Intelligence (AI)?\n",
    "\n",
    "Artificial Intelligence refers to the simulation of human intelligence\n",
    "in machines programmed to think and learn like humans. AI systems are\n",
    "designed to perform tasks such as visual perception, speech recognition,\n",
    "decision-making, and language translation.\n",
    "\n",
    "### What is Machine Learning (ML)?\n",
    "\n",
    "Machine Learning is a subset of AI that focuses on enabling machines to\n",
    "learn from data and improve their performance over time without being\n",
    "explicitly programmed for each task. It involves algorithms that\n",
    "identify patterns in data and make predictions or decisions.\n",
    "\n",
    "### What is Deep Learning?\n",
    "\n",
    "Deep Learning is a further subset of ML that uses artificial neural\n",
    "networks inspired by the human brain. These networks consist of layers\n",
    "that process data in complex ways, allowing the model to learn\n",
    "hierarchical representations and handle large amounts of unstructured\n",
    "data like images and text.\n",
    "\n",
    "### A Brief History\n",
    "\n",
    "-   **1950s**: Alan Turing introduced the concept of machines that could\n",
    "    mimic human intelligence, leading to the famous Turing Test.\n",
    "-   **1956**: John McCarthy coined the term ‘Artificial Intelligence’ at\n",
    "    the Dartmouth Conference, marking the birth of AI as a field.\n",
    "-   **1980s**: The revival of neural networks with the backpropagation\n",
    "    algorithm enabled better training of multi-layer networks.\n",
    "-   **2000s-Present**: Advances in computational power, data\n",
    "    availability, and algorithms led to breakthroughs in AI,\n",
    "    particularly in ML and Deep Learning.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Exploring New Orleans Short-Term Rental Licenses Dataset\n",
    "\n",
    "### Introduction to the Dataset\n",
    "\n",
    "We will analyze a dataset containing information about short-term rental\n",
    "licenses in New Orleans. This dataset includes details such as:\n",
    "\n",
    "-   License types and statuses\n",
    "-   Property types and locations\n",
    "-   Operator information\n",
    "-   Dates of application and issuance\n",
    "-   Rental unit characteristics (number of bedrooms, occupancy, etc.)\n",
    "\n",
    "Analyzing this data will help us understand patterns in short-term\n",
    "rentals across the city and demonstrate how machine learning can be\n",
    "applied to real-world data.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Importing Libraries\n",
    "\n",
    "Before we start, we need to import the necessary Python libraries for\n",
    "data manipulation, visualization, and machine learning.\n",
    "\n",
    "``` python\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning algorithms and tools\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Loading the Dataset\n",
    "\n",
    "Let’s load the dataset into a pandas DataFrame. Replace `'str_data.csv'`\n",
    "with the actual path to your CSV file.\n",
    "\n",
    "``` python\n",
    "# Load the dataset\n",
    "df = pd.read_csv('str_data.csv')\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "### Viewing the Data\n",
    "\n",
    "First, let’s take a look at the first few rows of the dataset to\n",
    "understand its structure.\n",
    "\n",
    "``` python\n",
    "# Display the first five rows\n",
    "df.head()\n",
    "```\n",
    "\n",
    "### Understanding the Data Structure\n",
    "\n",
    "We can get a summary of the DataFrame to see the data types and non-null\n",
    "counts.\n",
    "\n",
    "``` python\n",
    "# Get information about the DataFrame\n",
    "df.info()\n",
    "```\n",
    "\n",
    "### Summary Statistics\n",
    "\n",
    "Let’s view summary statistics for numerical columns.\n",
    "\n",
    "``` python\n",
    "# Summary statistics\n",
    "df.describe()\n",
    "```\n",
    "\n",
    "### Checking for Missing Values\n",
    "\n",
    "Identifying missing values is crucial for data cleaning.\n",
    "\n",
    "``` python\n",
    "# Check for missing values\n",
    "df.isnull().sum()\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "Data preprocessing involves cleaning and transforming data to prepare it\n",
    "for analysis.\n",
    "\n",
    "### Handling Missing Values\n",
    "\n",
    "We can choose to drop rows with missing values or fill them using\n",
    "appropriate methods.\n",
    "\n",
    "``` python\n",
    "# Drop rows with missing values\n",
    "df = df.dropna()\n",
    "```\n",
    "\n",
    "### Encoding Categorical Variables\n",
    "\n",
    "Machine learning models require numerical input. We’ll convert\n",
    "categorical variables into numerical form.\n",
    "\n",
    "#### Label Encoding\n",
    "\n",
    "Label Encoding converts categorical text data into model-understandable\n",
    "numerical data.\n",
    "\n",
    "``` python\n",
    "# Create a copy of the DataFrame\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# List of categorical columns to encode\n",
    "categorical_cols = ['Type', 'Current Status', 'Type of Building', 'Partial/Whole', 'Operator Type']\n",
    "\n",
    "# Apply LabelEncoder to each column\n",
    "for col in categorical_cols:\n",
    "    df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "```\n",
    "\n",
    "#### One-Hot Encoding\n",
    "\n",
    "Alternatively, One-Hot Encoding creates binary columns for each\n",
    "category.\n",
    "\n",
    "``` python\n",
    "# One-Hot Encoding using pandas\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols)\n",
    "```\n",
    "\n",
    "### Feature Selection\n",
    "\n",
    "Selecting relevant features is essential for building effective models.\n",
    "\n",
    "``` python\n",
    "# Define features (X) and target variable (y)\n",
    "# For example, let's predict 'Current Status' based on other features\n",
    "X = df_encoded.drop(['Current Status', 'License Number', 'Issue Date', 'Owner', 'Operator Email', 'Location'], axis=1)\n",
    "y = df_encoded['Current Status']\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Data Visualization\n",
    "\n",
    "Visualizing data helps uncover patterns and insights.\n",
    "\n",
    "### Distribution of License Types\n",
    "\n",
    "``` python\n",
    "# Plotting the distribution of license types\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.countplot(x='Type', data=df)\n",
    "plt.title('Distribution of License Types')\n",
    "plt.xlabel('License Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Number of Licenses by Building Type\n",
    "\n",
    "``` python\n",
    "# Plotting the number of licenses by building type\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(y='Type of Building', data=df, order=df['Type of Building'].value_counts().index)\n",
    "plt.title('Number of Licenses by Building Type')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Building Type')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Correlation Heatmap\n",
    "\n",
    "Understanding correlations between variables.\n",
    "\n",
    "``` python\n",
    "# Compute correlation matrix\n",
    "corr_matrix = df_encoded.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Machine Learning with Scikit-Learn\n",
    "\n",
    "We’ll build a Decision Tree Classifier to predict the ‘Current Status’\n",
    "of a license based on other features.\n",
    "\n",
    "### Splitting the Dataset\n",
    "\n",
    "Split the data into training and testing sets.\n",
    "\n",
    "``` python\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "```\n",
    "\n",
    "### Building and Training the Model\n",
    "\n",
    "``` python\n",
    "# Initialize the Decision Tree Classifier\n",
    "clf = Decision TreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "### Evaluating the Model\n",
    "\n",
    "``` python\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Interpreting the Results\n",
    "\n",
    "The classification report provides precision, recall, F1-score, and\n",
    "support for each class.\n",
    "\n",
    "-   **Precision**: The ability of the classifier not to label as\n",
    "    positive a sample that is negative.\n",
    "-   **Recall**: The ability of the classifier to find all the positive\n",
    "    samples.\n",
    "-   **F1-Score**: The harmonic mean of precision and recall.\n",
    "\n",
    "The confusion matrix shows the counts of true positives, false\n",
    "positives, true negatives, and false negatives.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Feature Importance\n",
    "\n",
    "Understanding which features influence the predictions.\n",
    "\n",
    "``` python\n",
    "# Get feature importances\n",
    "feature_importances = pd.Series(clf.feature_importances_, index=X.columns)\n",
    "\n",
    "# Sort and plot\n",
    "feature_importances.sort_values(ascending=False).plot(kind='bar', figsize=(12,6))\n",
    "plt.title('Feature Importance')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Improving the Model\n",
    "\n",
    "### Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "``` python\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Best estimator\n",
    "best_clf = grid_search.best_estimator_\n",
    "```\n",
    "\n",
    "### Evaluating the Tuned Model\n",
    "\n",
    "``` python\n",
    "# Predict with the best estimator\n",
    "y_pred_best = best_clf.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report for Best Model:\")\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "\n",
    "# Accuracy score\n",
    "best_accuracy = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"Best Model Accuracy: {best_accuracy:.2f}\")\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Building a Neural Network with Scikit-Learn\n",
    "\n",
    "We can also try a Neural Network model using `MLPClassifier`.\n",
    "\n",
    "### Training the Neural Network\n",
    "\n",
    "``` python\n",
    "# Initialize the MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=300, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "### Evaluating the Neural Network\n",
    "\n",
    "``` python\n",
    "# Make predictions\n",
    "y_pred_mlp = mlp.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report for Neural Network:\")\n",
    "print(classification_report(y_test, y_pred_mlp))\n",
    "\n",
    "# Accuracy score\n",
    "mlp_accuracy = accuracy_score(y_test, y_pred_mlp)\n",
    "print(f\"Neural Network Accuracy: {mlp_accuracy:.2f}\")\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Cross-Validation\n",
    "\n",
    "Using cross-validation to assess model performance more robustly.\n",
    "\n",
    "``` python\n",
    "# Cross-validation scores for the Decision Tree\n",
    "cv_scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(f\"Average Cross-Validation Score: {cv_scores.mean():.2f}\")\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "-   **Data Preprocessing**: Cleaning and encoding data is a crucial step\n",
    "    in preparing for machine learning.\n",
    "-   **Exploratory Data Analysis**: Visualizations help in understanding\n",
    "    the data and identifying patterns.\n",
    "-   **Model Building**: We built and evaluated a Decision Tree\n",
    "    Classifier and a Neural Network.\n",
    "-   **Model Improvement**: Hyperparameter tuning and cross-validation\n",
    "    can enhance model performance.\n",
    "-   **Feature Importance**: Identifying important features helps in\n",
    "    understanding what drives the predictions.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "-   **Further Feature Engineering**: Create new features or transform\n",
    "    existing ones to improve the model.\n",
    "-   **Address Class Imbalance**: If classes are imbalanced, consider\n",
    "    techniques like SMOTE or adjusting class weights.\n",
    "-   **Use More Advanced Models**: Explore other algorithms like Random\n",
    "    Forests, Gradient Boosting, or Support Vector Machines.\n",
    "-   **Deploy the Model**: Integrate the model into an application or\n",
    "    service for practical use.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Ethical Considerations\n",
    "\n",
    "When working with data, especially involving personal or sensitive\n",
    "information, it’s important to:\n",
    "\n",
    "-   **Ensure Data Privacy**: Handle data in compliance with relevant\n",
    "    laws and regulations.\n",
    "-   **Avoid Bias**: Be aware of potential biases in data and models that\n",
    "    could lead to unfair outcomes.\n",
    "-   **Transparency**: Be clear about how data is used and how models\n",
    "    make decisions.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Final Thoughts\n",
    "\n",
    "By analyzing the New Orleans Short-Term Rental Licenses dataset, we’ve\n",
    "demonstrated how machine learning can provide insights into real-world\n",
    "data. This process involves:\n",
    "\n",
    "1.  **Understanding the Problem**: Knowing what you want to predict or\n",
    "    analyze.\n",
    "2.  **Preparing the Data**: Cleaning and transforming data into a usable\n",
    "    format.\n",
    "3.  **Exploring the Data**: Using visualizations to uncover patterns.\n",
    "4.  **Building Models**: Training algorithms to learn from the data.\n",
    "5.  **Evaluating Models**: Assessing performance and making\n",
    "    improvements.\n",
    "6.  **Drawing Conclusions**: Interpreting the results to make informed\n",
    "    decisions.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "Thank you for exploring this introduction to AI and Machine Learning\n",
    "with a focus on New Orleans’ short-term rental data. We hope this\n",
    "provides a foundation for further learning and application in your own\n",
    "projects!"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
